#!/usr/bin/env python
"""
ç»˜åˆ¶é¢„æµ‹ç»“æœå›¾
ä» predictions_*.csv ç”Ÿæˆé¢„æµ‹vsçœŸå®å€¼çš„æ•£ç‚¹å›¾
"""

import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
plt.rcParams['legend.fontsize'] = 11
plt.rcParams['figure.titlesize'] = 18


def calculate_regression_metrics(y_true, y_pred):
    """è®¡ç®—å›å½’æŒ‡æ ‡"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    return {
        'MAE': mae,
        'RMSE': rmse,
        'RÂ²': r2
    }


def calculate_classification_metrics(y_true, y_pred, y_pred_proba=None):
    """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
    # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°
    y_true = y_true.astype(int)
    y_pred_binary = (y_pred > 0.5).astype(int)

    metrics = {
        'Accuracy': accuracy_score(y_true, y_pred_binary),
        'Precision': precision_score(y_true, y_pred_binary, zero_division=0),
        'Recall': recall_score(y_true, y_pred_binary, zero_division=0),
        'F1': f1_score(y_true, y_pred_binary, zero_division=0)
    }

    # å¦‚æœæä¾›äº†æ¦‚ç‡é¢„æµ‹ï¼Œè®¡ç®—AUC
    if y_pred_proba is not None or len(np.unique(y_pred)) > 2:
        try:
            metrics['AUC-ROC'] = roc_auc_score(y_true, y_pred)
        except:
            pass

    return metrics


def plot_regression_predictions(output_dir, save_dir=None, show=True):
    """
    ç»˜åˆ¶å›å½’é¢„æµ‹ç»“æœ

    Args:
        output_dir: åŒ…å«predictions_*.csvæ–‡ä»¶çš„ç›®å½•
        save_dir: ä¿å­˜å›¾ç‰‡çš„ç›®å½•ï¼ˆå¦‚æœNoneåˆ™ä¿å­˜åˆ°output_dirï¼‰
        show: æ˜¯å¦æ˜¾ç¤ºå›¾ç‰‡
    """
    if save_dir is None:
        save_dir = output_dir
    os.makedirs(save_dir, exist_ok=True)

    # æŸ¥æ‰¾é¢„æµ‹æ–‡ä»¶
    pred_files = {
        'train': os.path.join(output_dir, 'predictions_train.csv'),
        'val': os.path.join(output_dir, 'predictions_val.csv'),
        'test': os.path.join(output_dir, 'predictions_test.csv')
    }

    # æ£€æŸ¥å“ªäº›æ–‡ä»¶å­˜åœ¨
    available_sets = {}
    for set_name, file_path in pred_files.items():
        if os.path.exists(file_path):
            available_sets[set_name] = file_path
            print(f"âœ… æ‰¾åˆ°{set_name}é›†é¢„æµ‹æ–‡ä»¶: {file_path}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°{set_name}é›†é¢„æµ‹æ–‡ä»¶: {file_path}")

    if not available_sets:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•é¢„æµ‹æ–‡ä»¶ï¼")
        return

    # è¯»å–æ•°æ®
    data = {}
    for set_name, file_path in available_sets.items():
        df = pd.read_csv(file_path)
        print(f"   {set_name}é›†æ ·æœ¬æ•°: {len(df)}")
        data[set_name] = df

    # æ£€æµ‹æ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’ä»»åŠ¡
    # é€šè¿‡æ£€æŸ¥é¢„æµ‹å€¼çš„å”¯ä¸€å€¼æ•°é‡æ¥åˆ¤æ–­
    first_df = list(data.values())[0]
    unique_targets = first_df['target'].nunique()

    is_classification = unique_targets <= 10 and set(first_df['target'].unique()).issubset({0, 1})

    if is_classification:
        print("\nğŸ“Š æ£€æµ‹åˆ°åˆ†ç±»ä»»åŠ¡")
        plot_classification_predictions(data, save_dir, show)
    else:
        print("\nğŸ“Š æ£€æµ‹åˆ°å›å½’ä»»åŠ¡")
        plot_regression_scatter(data, save_dir, show)


def plot_regression_scatter(data, save_dir, show):
    """ç»˜åˆ¶å›å½’ä»»åŠ¡çš„æ•£ç‚¹å›¾"""

    n_sets = len(data)

    # ========== å›¾1: ä¸‰ä¸ªå­å›¾åˆ†åˆ«æ˜¾ç¤º ==========
    fig, axes = plt.subplots(1, n_sets, figsize=(6*n_sets, 5))
    if n_sets == 1:
        axes = [axes]

    metrics_summary = {}

    for idx, (set_name, df) in enumerate(data.items()):
        ax = axes[idx]

        y_true = df['target'].values
        y_pred = df['prediction'].values

        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_regression_metrics(y_true, y_pred)
        metrics_summary[set_name] = metrics

        # ç»˜åˆ¶æ•£ç‚¹å›¾
        ax.scatter(y_true, y_pred, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)

        # æ·»åŠ å¯¹è§’çº¿ (y=x)
        min_val = min(y_true.min(), y_pred.min())
        max_val = max(y_true.max(), y_pred.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='y=x')

        # è®¾ç½®æ ‡ç­¾
        ax.set_xlabel('True Value')
        ax.set_ylabel('Predicted Value')
        ax.set_title(f'{set_name.capitalize()} Set')

        # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
        textstr = f"MAE = {metrics['MAE']:.4f}\nRMSE = {metrics['RMSE']:.4f}\nRÂ² = {metrics['RÂ²']:.4f}"
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,
                verticalalignment='top', bbox=props)

        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    separate_path = os.path.join(save_dir, 'predictions_separate.pdf')
    plt.savefig(separate_path, dpi=300, bbox_inches='tight')
    plt.savefig(separate_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜åˆ†å¼€çš„é¢„æµ‹å›¾: {separate_path}")

    if show:
        plt.show()
    plt.close()

    # ========== å›¾2: åˆå¹¶åœ¨ä¸€ä¸ªå›¾ä¸­ ==========
    fig, ax = plt.subplots(figsize=(10, 8))

    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # train, val, test
    markers = ['o', 's', '^']

    for idx, (set_name, df) in enumerate(data.items()):
        y_true = df['target'].values
        y_pred = df['prediction'].values

        metrics = metrics_summary[set_name]
        label = f"{set_name.capitalize()}: MAE={metrics['MAE']:.4f}, RÂ²={metrics['RÂ²']:.3f}"

        ax.scatter(y_true, y_pred, alpha=0.5, s=40,
                  color=colors[idx], marker=markers[idx],
                  edgecolors='k', linewidth=0.5, label=label)

    # æ·»åŠ å¯¹è§’çº¿
    all_true = np.concatenate([df['target'].values for df in data.values()])
    all_pred = np.concatenate([df['prediction'].values for df in data.values()])
    min_val = min(all_true.min(), all_pred.min())
    max_val = max(all_true.max(), all_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, alpha=0.7, label='y=x')

    ax.set_xlabel('True Value')
    ax.set_ylabel('Predicted Value')
    ax.set_title('Predictions vs. True Values (All Sets)')
    ax.legend(loc='best', fontsize=10)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    combined_path = os.path.join(save_dir, 'predictions_combined.pdf')
    plt.savefig(combined_path, dpi=300, bbox_inches='tight')
    plt.savefig(combined_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜åˆå¹¶çš„é¢„æµ‹å›¾: {combined_path}")

    if show:
        plt.show()
    plt.close()

    # ========== å›¾3: æ®‹å·®å›¾ ==========
    fig, axes = plt.subplots(1, n_sets, figsize=(6*n_sets, 5))
    if n_sets == 1:
        axes = [axes]

    for idx, (set_name, df) in enumerate(data.items()):
        ax = axes[idx]

        y_true = df['target'].values
        y_pred = df['prediction'].values
        residuals = y_true - y_pred

        # æ®‹å·®æ•£ç‚¹å›¾
        ax.scatter(y_pred, residuals, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
        ax.axhline(y=0, color='r', linestyle='--', linewidth=2)

        ax.set_xlabel('Predicted Value')
        ax.set_ylabel('Residuals (True - Predicted)')
        ax.set_title(f'{set_name.capitalize()} Set - Residuals')
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    residuals_path = os.path.join(save_dir, 'predictions_residuals.pdf')
    plt.savefig(residuals_path, dpi=300, bbox_inches='tight')
    plt.savefig(residuals_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜æ®‹å·®å›¾: {residuals_path}")

    if show:
        plt.show()
    plt.close()

    # ========== æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ==========
    print("\n" + "="*70)
    print("ğŸ“Š å›å½’é¢„æµ‹ç»Ÿè®¡ä¿¡æ¯")
    print("="*70)
    for set_name, metrics in metrics_summary.items():
        print(f"\n{set_name.upper()} SET:")
        print(f"  æ ·æœ¬æ•°: {len(data[set_name])}")
        print(f"  MAE:    {metrics['MAE']:.6f}")
        print(f"  RMSE:   {metrics['RMSE']:.6f}")
        print(f"  RÂ²:     {metrics['RÂ²']:.6f}")
    print("="*70 + "\n")


def plot_classification_predictions(data, save_dir, show):
    """ç»˜åˆ¶åˆ†ç±»ä»»åŠ¡çš„é¢„æµ‹ç»“æœ"""

    n_sets = len(data)

    # ========== å›¾1: æ··æ·†çŸ©é˜µ ==========
    fig, axes = plt.subplots(1, n_sets, figsize=(6*n_sets, 5))
    if n_sets == 1:
        axes = [axes]

    metrics_summary = {}

    for idx, (set_name, df) in enumerate(data.items()):
        ax = axes[idx]

        y_true = df['target'].values.astype(int)
        y_pred = (df['prediction'].values > 0.5).astype(int)

        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_classification_metrics(y_true, df['prediction'].values)
        metrics_summary[set_name] = metrics

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    cbar=True, square=True, annot_kws={"size": 16})
        ax.set_xlabel('Predicted Label')
        ax.set_ylabel('True Label')
        ax.set_title(f'{set_name.capitalize()} Set - Confusion Matrix')

    plt.tight_layout()
    cm_path = os.path.join(save_dir, 'predictions_confusion_matrix.pdf')
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    plt.savefig(cm_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜æ··æ·†çŸ©é˜µ: {cm_path}")

    if show:
        plt.show()
    plt.close()

    # ========== å›¾2: é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ ==========
    fig, axes = plt.subplots(1, n_sets, figsize=(6*n_sets, 5))
    if n_sets == 1:
        axes = [axes]

    for idx, (set_name, df) in enumerate(data.items()):
        ax = axes[idx]

        y_true = df['target'].values.astype(int)
        y_pred_proba = df['prediction'].values

        # åˆ†åˆ«ç»˜åˆ¶ä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
        class_0_probs = y_pred_proba[y_true == 0]
        class_1_probs = y_pred_proba[y_true == 1]

        ax.hist(class_0_probs, bins=30, alpha=0.6, label='Class 0 (True)', color='blue', edgecolor='black')
        ax.hist(class_1_probs, bins=30, alpha=0.6, label='Class 1 (True)', color='red', edgecolor='black')
        ax.axvline(x=0.5, color='green', linestyle='--', linewidth=2, label='Threshold=0.5')

        ax.set_xlabel('Predicted Probability')
        ax.set_ylabel('Count')
        ax.set_title(f'{set_name.capitalize()} Set - Prediction Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    dist_path = os.path.join(save_dir, 'predictions_probability_distribution.pdf')
    plt.savefig(dist_path, dpi=300, bbox_inches='tight')
    plt.savefig(dist_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜æ¦‚ç‡åˆ†å¸ƒå›¾: {dist_path}")

    if show:
        plt.show()
    plt.close()

    # ========== å›¾3: ROCæ›²çº¿ï¼ˆå¦‚æœå¯è®¡ç®—ï¼‰ ==========
    try:
        from sklearn.metrics import roc_curve, auc

        fig, ax = plt.subplots(figsize=(8, 8))

        for set_name, df in data.items():
            y_true = df['target'].values.astype(int)
            y_pred_proba = df['prediction'].values

            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
            roc_auc = auc(fpr, tpr)

            ax.plot(fpr, tpr, linewidth=2,
                   label=f'{set_name.capitalize()} (AUC = {roc_auc:.3f})')

        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title('ROC Curves')
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        roc_path = os.path.join(save_dir, 'predictions_roc_curve.pdf')
        plt.savefig(roc_path, dpi=300, bbox_inches='tight')
        plt.savefig(roc_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
        print(f"âœ… ä¿å­˜ROCæ›²çº¿: {roc_path}")

        if show:
            plt.show()
        plt.close()
    except Exception as e:
        print(f"âš ï¸  æ— æ³•ç»˜åˆ¶ROCæ›²çº¿: {e}")

    # ========== æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ==========
    print("\n" + "="*70)
    print("ğŸ“Š åˆ†ç±»é¢„æµ‹ç»Ÿè®¡ä¿¡æ¯")
    print("="*70)
    for set_name, metrics in metrics_summary.items():
        print(f"\n{set_name.upper()} SET:")
        print(f"  æ ·æœ¬æ•°:   {len(data[set_name])}")
        print(f"  Accuracy:  {metrics['Accuracy']:.6f}")
        print(f"  Precision: {metrics['Precision']:.6f}")
        print(f"  Recall:    {metrics['Recall']:.6f}")
        print(f"  F1 Score:  {metrics['F1']:.6f}")
        if 'AUC-ROC' in metrics:
            print(f"  AUC-ROC:   {metrics['AUC-ROC']:.6f}")
    print("="*70 + "\n")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ç»˜åˆ¶é¢„æµ‹ç»“æœå›¾')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='åŒ…å«predictions_*.csvæ–‡ä»¶çš„è¾“å‡ºç›®å½•')
    parser.add_argument('--save_dir', type=str, default=None,
                        help='ä¿å­˜å›¾ç‰‡çš„ç›®å½•ï¼ˆé»˜è®¤ä¸output_dirç›¸åŒï¼‰')
    parser.add_argument('--no_show', action='store_true',
                        help='ä¸æ˜¾ç¤ºå›¾ç‰‡ï¼ˆä»…ä¿å­˜ï¼‰')

    args = parser.parse_args()

    print("="*60)
    print("ğŸ“ˆ å¼€å§‹ç»˜åˆ¶é¢„æµ‹ç»“æœå›¾")
    print("="*60)
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir or args.output_dir}")
    print()

    plot_regression_predictions(
        output_dir=args.output_dir,
        save_dir=args.save_dir,
        show=not args.no_show
    )

    print("âœ… æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
