#!/usr/bin/env python
"""
ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿
ä» history_train.json å’Œ history_val.json ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
"""

import os
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
plt.rcParams['legend.fontsize'] = 12
plt.rcParams['figure.titlesize'] = 18


def plot_training_history(output_dir, save_dir=None, show=True):
    """
    ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿

    Args:
        output_dir: åŒ…å«history_*.jsonæ–‡ä»¶çš„ç›®å½•
        save_dir: ä¿å­˜å›¾ç‰‡çš„ç›®å½•ï¼ˆå¦‚æœNoneåˆ™ä¿å­˜åˆ°output_dirï¼‰
        show: æ˜¯å¦æ˜¾ç¤ºå›¾ç‰‡
    """
    if save_dir is None:
        save_dir = output_dir
    os.makedirs(save_dir, exist_ok=True)

    # åŠ è½½è®­ç»ƒå’ŒéªŒè¯å†å²
    train_history_file = os.path.join(output_dir, 'history_train.json')
    val_history_file = os.path.join(output_dir, 'history_val.json')

    if not os.path.exists(train_history_file):
        print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒå†å²æ–‡ä»¶: {train_history_file}")
        return
    if not os.path.exists(val_history_file):
        print(f"âŒ æœªæ‰¾åˆ°éªŒè¯å†å²æ–‡ä»¶: {val_history_file}")
        return

    with open(train_history_file, 'r') as f:
        train_history = json.load(f)
    with open(val_history_file, 'r') as f:
        val_history = json.load(f)

    print(f"âœ… åŠ è½½å†å²æ•°æ®æˆåŠŸ")
    print(f"   è®­ç»ƒæŒ‡æ ‡: {list(train_history.keys())}")
    print(f"   éªŒè¯æŒ‡æ ‡: {list(val_history.keys())}")

    # æ£€æµ‹ä»»åŠ¡ç±»å‹ï¼ˆåˆ†ç±»æˆ–å›å½’ï¼‰
    metrics_list = list(train_history.keys())
    is_classification = 'accuracy' in metrics_list

    if is_classification:
        print(f"ğŸ“Š æ£€æµ‹åˆ°åˆ†ç±»ä»»åŠ¡")
        main_metric = 'accuracy'
        metric_label = 'Accuracy'
        metric_better = 'higher'
    else:
        print(f"ğŸ“Š æ£€æµ‹åˆ°å›å½’ä»»åŠ¡")
        main_metric = 'mae'
        metric_label = 'MAE'
        metric_better = 'lower'

    epochs = list(range(1, len(train_history['loss']) + 1))

    # ========== å›¾1: Lossæ›²çº¿ ==========
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.plot(epochs, train_history['loss'],
            label='Training Loss', linewidth=2, marker='o', markersize=4,
            markevery=max(1, len(epochs)//20))
    ax.plot(epochs, val_history['loss'],
            label='Validation Loss', linewidth=2, marker='s', markersize=4,
            markevery=max(1, len(epochs)//20))

    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('Training and Validation Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # æ ‡æ³¨æœ€å°éªŒè¯æŸå¤±
    min_val_loss_epoch = np.argmin(val_history['loss']) + 1
    min_val_loss = min(val_history['loss'])
    ax.axvline(x=min_val_loss_epoch, color='red', linestyle='--',
               alpha=0.5, label=f'Best Val Loss (Epoch {min_val_loss_epoch})')
    ax.legend()

    plt.tight_layout()
    loss_fig_path = os.path.join(save_dir, 'training_loss_curve.pdf')
    plt.savefig(loss_fig_path, dpi=300, bbox_inches='tight')
    plt.savefig(loss_fig_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜Lossæ›²çº¿: {loss_fig_path}")

    if show:
        plt.show()
    plt.close()

    # ========== å›¾2: ä¸»è¦æŒ‡æ ‡æ›²çº¿ (MAEæˆ–Accuracy) ==========
    if main_metric in train_history:
        fig, ax = plt.subplots(figsize=(10, 6))

        ax.plot(epochs, train_history[main_metric],
                label=f'Training {metric_label}', linewidth=2, marker='o',
                markersize=4, markevery=max(1, len(epochs)//20))
        ax.plot(epochs, val_history[main_metric],
                label=f'Validation {metric_label}', linewidth=2, marker='s',
                markersize=4, markevery=max(1, len(epochs)//20))

        ax.set_xlabel('Epoch')
        ax.set_ylabel(metric_label)
        ax.set_title(f'Training and Validation {metric_label}')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # æ ‡æ³¨æœ€ä½³æŒ‡æ ‡
        if metric_better == 'lower':
            best_val_epoch = np.argmin(val_history[main_metric]) + 1
            best_val_metric = min(val_history[main_metric])
        else:
            best_val_epoch = np.argmax(val_history[main_metric]) + 1
            best_val_metric = max(val_history[main_metric])

        ax.axvline(x=best_val_epoch, color='red', linestyle='--',
                   alpha=0.5, label=f'Best Val (Epoch {best_val_epoch})')
        ax.legend()

        plt.tight_layout()
        metric_fig_path = os.path.join(save_dir, f'training_{main_metric}_curve.pdf')
        plt.savefig(metric_fig_path, dpi=300, bbox_inches='tight')
        plt.savefig(metric_fig_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
        print(f"âœ… ä¿å­˜{metric_label}æ›²çº¿: {metric_fig_path}")

        if show:
            plt.show()
        plt.close()

    # ========== å›¾3: åˆ†ç±»ä»»åŠ¡é¢å¤–æŒ‡æ ‡ (Precision, Recall) ==========
    if is_classification and 'precision' in train_history and 'recall' in train_history:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # Precision
        ax1.plot(epochs, train_history['precision'],
                label='Training Precision', linewidth=2, marker='o', markersize=4,
                markevery=max(1, len(epochs)//20))
        ax1.plot(epochs, val_history['precision'],
                label='Validation Precision', linewidth=2, marker='s', markersize=4,
                markevery=max(1, len(epochs)//20))
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Precision')
        ax1.set_title('Training and Validation Precision')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Recall
        ax2.plot(epochs, train_history['recall'],
                label='Training Recall', linewidth=2, marker='o', markersize=4,
                markevery=max(1, len(epochs)//20))
        ax2.plot(epochs, val_history['recall'],
                label='Validation Recall', linewidth=2, marker='s', markersize=4,
                markevery=max(1, len(epochs)//20))
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Recall')
        ax2.set_title('Training and Validation Recall')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        class_metrics_path = os.path.join(save_dir, 'training_classification_metrics.pdf')
        plt.savefig(class_metrics_path, dpi=300, bbox_inches='tight')
        plt.savefig(class_metrics_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
        print(f"âœ… ä¿å­˜åˆ†ç±»æŒ‡æ ‡æ›²çº¿: {class_metrics_path}")

        if show:
            plt.show()
        plt.close()

    # ========== å›¾4: ç»¼åˆå¯¹æ¯”å›¾ ==========
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # ä¸Šå›¾ï¼šLoss
    axes[0].plot(epochs, train_history['loss'],
                label='Training Loss', linewidth=2, alpha=0.8)
    axes[0].plot(epochs, val_history['loss'],
                label='Validation Loss', linewidth=2, alpha=0.8)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss Curves')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # ä¸‹å›¾ï¼šä¸»è¦æŒ‡æ ‡
    if main_metric in train_history:
        axes[1].plot(epochs, train_history[main_metric],
                    label=f'Training {metric_label}', linewidth=2, alpha=0.8)
        axes[1].plot(epochs, val_history[main_metric],
                    label=f'Validation {metric_label}', linewidth=2, alpha=0.8)
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel(metric_label)
        axes[1].set_title(f'{metric_label} Curves')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    combined_path = os.path.join(save_dir, 'training_curves_combined.pdf')
    plt.savefig(combined_path, dpi=300, bbox_inches='tight')
    plt.savefig(combined_path.replace('.pdf', '.png'), dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜ç»¼åˆæ›²çº¿: {combined_path}")

    if show:
        plt.show()
    plt.close()

    # ========== æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ==========
    print("\n" + "="*60)
    print("ğŸ“Š è®­ç»ƒç»Ÿè®¡ä¿¡æ¯")
    print("="*60)
    print(f"æ€»è®­ç»ƒè½®æ¬¡: {len(epochs)}")
    print(f"æœ€ç»ˆè®­ç»ƒLoss: {train_history['loss'][-1]:.6f}")
    print(f"æœ€ç»ˆéªŒè¯Loss: {val_history['loss'][-1]:.6f}")
    print(f"æœ€å°éªŒè¯Loss: {min_val_loss:.6f} (Epoch {min_val_loss_epoch})")

    if main_metric in train_history:
        print(f"\næœ€ç»ˆè®­ç»ƒ{metric_label}: {train_history[main_metric][-1]:.6f}")
        print(f"æœ€ç»ˆéªŒè¯{metric_label}: {val_history[main_metric][-1]:.6f}")
        print(f"æœ€ä½³éªŒè¯{metric_label}: {best_val_metric:.6f} (Epoch {best_val_epoch})")

    if is_classification:
        if 'precision' in val_history and 'recall' in val_history:
            print(f"\næœ€ç»ˆéªŒè¯Precision: {val_history['precision'][-1]:.6f}")
            print(f"æœ€ç»ˆéªŒè¯Recall: {val_history['recall'][-1]:.6f}")
    print("="*60 + "\n")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='åŒ…å«history_*.jsonæ–‡ä»¶çš„è¾“å‡ºç›®å½•')
    parser.add_argument('--save_dir', type=str, default=None,
                        help='ä¿å­˜å›¾ç‰‡çš„ç›®å½•ï¼ˆé»˜è®¤ä¸output_dirç›¸åŒï¼‰')
    parser.add_argument('--no_show', action='store_true',
                        help='ä¸æ˜¾ç¤ºå›¾ç‰‡ï¼ˆä»…ä¿å­˜ï¼‰')

    args = parser.parse_args()

    print("="*60)
    print("ğŸ“ˆ å¼€å§‹ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿")
    print("="*60)
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir or args.output_dir}")
    print()

    plot_training_history(
        output_dir=args.output_dir,
        save_dir=args.save_dir,
        show=not args.no_show
    )

    print("âœ… æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
